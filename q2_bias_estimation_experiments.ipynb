{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from ax import optimize\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.utils.notebook.plotting import init_notebook_plotting, render\n",
    "from ax.plot.trace import optimization_trace_single_method\n",
    "\n",
    "import json\n",
    "import torch\n",
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Dataset\n",
    "\n",
    "Let's start with a ground-truth linear model that we know a linear model should be able to recover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dataset\n",
    "NROW = 1000\n",
    "NCOL = 5\n",
    "\n",
    "random.seed(1010)\n",
    "\n",
    "data = np.random.rand(NROW, NCOL)\n",
    "X = pd.DataFrame(data, columns=[f\"col_{i}\" for i in range(1, NCOL+1)])\n",
    "y = X[\"col_1\"] + 2 * X[\"col_2\"] + 3 * X[\"col_3\"] + 4 * X[\"col_4\"] + 5 * X[\"col_5\"] + np.random.uniform(0, 0.5, NROW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1010)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize/Train the Model on the Training Set (and report optimal hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from Mohammed's q2 optimization notebook\n",
    "model_dict = {\"RandomForestRegressor\":{\"model\":RandomForestRegressor,\n",
    "\t\t\t\t\t\"params\":[{\"name\":\"n_estimators\", \"type\":\"range\", \"bounds\":[2,200]},\n",
    "\t\t\t\t\t\t\t{\"name\":\"max_depth\", \"type\":\"range\", \"bounds\":[1,10]},\n",
    "\t\t\t\t\t\t\t{\"name\":\"min_samples_leaf\", \"type\":\"range\", \"bounds\":[1,5]}]},\n",
    "\t\t\t   \"ElasticNet\":{ \"model\": ElasticNet,\n",
    "\t\t\t\t\t\"params\":[{\"name\": \"alpha\", \"type\": \"range\", \"bounds\":[0.001,1.0]},\n",
    "\t\t\t\t\t\t\t{\"name\": \"l1_ratio\", \"type\": \"range\", \"bounds\":[0.0,1.0]},\n",
    "\t\t\t\t\t\t\t{\"name\": \"max_iter\", \"type\": \"range\", \"bounds\":[200, 2000]},\n",
    "\t\t\t\t\t\t\t{\"name\": \"selection\", \"type\": \"choice\", \"values\":[\"cyclic\", \"random\"]}\n",
    "\t\t\t\t\t\t]}\n",
    "\t\t\t}\n",
    "\n",
    "def q2_baseline_models(estimator, X, y):\n",
    "    baseline_models = []\n",
    "    for loo_index in X.index:\n",
    "        if hasattr(estimator, \"random_state\"):\n",
    "            estimator.random_state = loo_index\t\n",
    "        baseline_models.append(deepcopy(estimator.fit(X=X.drop(loo_index).values, y=y.drop(loo_index).values)))\n",
    "        \n",
    "    return baseline_models\n",
    "\n",
    "# This function computes q^2 (used as evaluation for the models)\n",
    "def q2_score(estimator, X, y):\n",
    "    models = q2_baseline_models(estimator, X, y)\n",
    "    q2_means = []\n",
    "    q2_preds = []\n",
    "    \n",
    "    for loo_index in X.index:\n",
    "        q2_means.append(y.drop(loo_index).mean())\n",
    "        q2_preds.append(models[loo_index].predict(np.array(X.iloc[loo_index]).reshape(1,-1))[0])\n",
    "\n",
    "    q2 = 1 - np.sum((np.array(q2_preds) - np.array(y))**2) / np.sum((np.array(q2_means) - np.array(y))**2)\n",
    "    \n",
    "    return q2\n",
    "\n",
    "def get_optimal_model(X, y, model_type, total_trials):\n",
    "    best_parameters, best_values, experiment, model = optimize(\n",
    "        parameters= model_dict[model_type][\"params\"],\n",
    "        evaluation_function=lambda p: q2_score(model_dict[model_type][\"model\"](**p), X, y),\n",
    "        minimize=False,\n",
    "        total_trials=total_trials,\n",
    "    )\n",
    "    optimal_parameters = best_parameters \n",
    "    optimization_q2 = best_values[0][\"objective\"]\n",
    "\n",
    "    return(optimal_parameters, optimization_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-26 15:53:55] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter alpha. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 03-26 15:53:55] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter l1_ratio. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 03-26 15:53:55] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter max_iter. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 03-26 15:53:55] ax.service.utils.instantiation: Inferred value type of ParameterType.STRING for parameter selection. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "/Users/xehu/anaconda3/envs/tpm_horserace/lib/python3.11/site-packages/ax/core/parameter.py:518: UserWarning:\n",
      "\n",
      "`is_ordered` is not specified for `ChoiceParameter` \"selection\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `is_ordered` during `ChoiceParameter` construction.\n",
      "\n",
      "/Users/xehu/anaconda3/envs/tpm_horserace/lib/python3.11/site-packages/ax/core/parameter.py:518: UserWarning:\n",
      "\n",
      "`sort_values` is not specified for `ChoiceParameter` \"selection\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "\n",
      "[INFO 03-26 15:53:55] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='alpha', parameter_type=FLOAT, range=[0.001, 1.0]), RangeParameter(name='l1_ratio', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='max_iter', parameter_type=INT, range=[200, 2000]), ChoiceParameter(name='selection', parameter_type=STRING, values=['cyclic', 'random'], is_ordered=False, sort_values=False)], parameter_constraints=[]).\n",
      "[INFO 03-26 15:53:55] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there are more ordered parameters than there are categories for the unordered categorical parameters.\n",
      "[INFO 03-26 15:53:55] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=4 num_trials=None use_batch_trials=False\n",
      "[INFO 03-26 15:53:55] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=8\n",
      "[INFO 03-26 15:53:55] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=8\n",
      "[INFO 03-26 15:53:55] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 03-26 15:53:55] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 8 trials, BoTorch for subsequent trials]). Iterations after 8 will take longer to generate due to model-fitting.\n",
      "[INFO 03-26 15:53:55] ax.service.managed_loop: Started full optimization with 10 steps.\n",
      "[INFO 03-26 15:53:55] ax.service.managed_loop: Running optimization trial 1...\n",
      "[INFO 03-26 15:53:55] ax.service.managed_loop: Running optimization trial 2...\n",
      "[INFO 03-26 15:53:56] ax.service.managed_loop: Running optimization trial 3...\n",
      "[INFO 03-26 15:53:56] ax.service.managed_loop: Running optimization trial 4...\n",
      "[INFO 03-26 15:53:56] ax.service.managed_loop: Running optimization trial 5...\n",
      "[INFO 03-26 15:53:56] ax.service.managed_loop: Running optimization trial 6...\n",
      "[INFO 03-26 15:53:57] ax.service.managed_loop: Running optimization trial 7...\n",
      "[INFO 03-26 15:53:57] ax.service.managed_loop: Running optimization trial 8...\n",
      "[INFO 03-26 15:53:57] ax.service.managed_loop: Running optimization trial 9...\n",
      "[INFO 03-26 15:53:58] ax.service.managed_loop: Running optimization trial 10...\n",
      "/Users/xehu/anaconda3/envs/tpm_horserace/lib/python3.11/site-packages/ax/utils/stats/model_fit_stats.py:133: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n",
      "/Users/xehu/anaconda3/envs/tpm_horserace/lib/python3.11/site-packages/ax/utils/stats/model_fit_stats.py:158: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n",
      "[WARNING 03-26 15:53:59] ax.modelbridge.cross_validation: Metric objective was unable to be reliably fit.\n",
      "[WARNING 03-26 15:53:59] ax.service.utils.best_point: Model fit is poor; falling back on raw data for best point.\n",
      "[WARNING 03-26 15:53:59] ax.service.utils.best_point: Model fit is poor and data on objective metric objective is noisy; interpret best points results carefully.\n"
     ]
    }
   ],
   "source": [
    "# print the training q^2 on the training dataset\n",
    "optimal_params, optimal_q2 = get_optimal_model(X_train, y_train, model_type = \"ElasticNet\", total_trials = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9954762787725429"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.001,\n",
       " 'l1_ratio': 0.5135116582779522,\n",
       " 'max_iter': 1925,\n",
       " 'selection': 'cyclic'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on the true held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the evaluation q^2 on the evaluation dataset\n",
    "model_optimal = ElasticNet(alpha = optimal_params[\"alpha\"],\n",
    "                           l1_ratio= optimal_params[\"l1_ratio\"],\n",
    "                           max_iter = optimal_params[\"max_iter\"],\n",
    "                           selection = optimal_params[\"selection\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.01194055, 2.03002156, 2.9994982 , 3.9544421 , 4.94892631])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that the model's coefficients recover the ground truth\n",
    "model_optimal.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprise --- we found the ground truth coefficients, so it works out-of-sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9952759010180788"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_score(model_optimal, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Dataset with Omitted Variables and Nonlinearities\n",
    "\n",
    "Let's create a dataset with nonlinear relationships, omitted variables, and lots of noise --- making it more likely that the original model will overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dataset\n",
    "NROW = 1000\n",
    "NCOL = 5\n",
    "\n",
    "random.seed(1010)\n",
    "\n",
    "data = np.random.rand(NROW, NCOL)\n",
    "X = pd.DataFrame(data, columns=[f\"col_{i}\" for i in range(1, NCOL+1)])\n",
    "y = X[\"col_1\"] + 2 * X[\"col_2\"] + 3 * X[\"col_3\"]**3 + 4 * X[\"col_4\"] * X[\"col_5\"] + np.random.uniform(0, 2, NROW)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1010)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# drop cols 1 and 5 so that we have omitted variables\n",
    "X_train = X_train.drop([\"col_1\", \"col_5\"], axis = 1)\n",
    "X_test = X_test.drop([\"col_1\", \"col_5\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-26 16:04:43] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter alpha. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 03-26 16:04:43] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter l1_ratio. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 03-26 16:04:43] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter max_iter. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 03-26 16:04:43] ax.service.utils.instantiation: Inferred value type of ParameterType.STRING for parameter selection. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "/Users/xehu/anaconda3/envs/tpm_horserace/lib/python3.11/site-packages/ax/core/parameter.py:518: UserWarning:\n",
      "\n",
      "`is_ordered` is not specified for `ChoiceParameter` \"selection\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `is_ordered` during `ChoiceParameter` construction.\n",
      "\n",
      "/Users/xehu/anaconda3/envs/tpm_horserace/lib/python3.11/site-packages/ax/core/parameter.py:518: UserWarning:\n",
      "\n",
      "`sort_values` is not specified for `ChoiceParameter` \"selection\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "\n",
      "[INFO 03-26 16:04:43] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='alpha', parameter_type=FLOAT, range=[0.001, 1.0]), RangeParameter(name='l1_ratio', parameter_type=FLOAT, range=[0.0, 1.0]), RangeParameter(name='max_iter', parameter_type=INT, range=[200, 2000]), ChoiceParameter(name='selection', parameter_type=STRING, values=['cyclic', 'random'], is_ordered=False, sort_values=False)], parameter_constraints=[]).\n",
      "[INFO 03-26 16:04:43] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there are more ordered parameters than there are categories for the unordered categorical parameters.\n",
      "[INFO 03-26 16:04:43] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=4 num_trials=None use_batch_trials=False\n",
      "[INFO 03-26 16:04:43] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=8\n",
      "[INFO 03-26 16:04:43] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=8\n",
      "[INFO 03-26 16:04:43] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 03-26 16:04:43] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 8 trials, BoTorch for subsequent trials]). Iterations after 8 will take longer to generate due to model-fitting.\n",
      "[INFO 03-26 16:04:43] ax.service.managed_loop: Started full optimization with 10 steps.\n",
      "[INFO 03-26 16:04:43] ax.service.managed_loop: Running optimization trial 1...\n",
      "[INFO 03-26 16:04:44] ax.service.managed_loop: Running optimization trial 2...\n",
      "[INFO 03-26 16:04:44] ax.service.managed_loop: Running optimization trial 3...\n",
      "[INFO 03-26 16:04:44] ax.service.managed_loop: Running optimization trial 4...\n",
      "[INFO 03-26 16:04:45] ax.service.managed_loop: Running optimization trial 5...\n",
      "[INFO 03-26 16:04:45] ax.service.managed_loop: Running optimization trial 6...\n",
      "[INFO 03-26 16:04:45] ax.service.managed_loop: Running optimization trial 7...\n",
      "[INFO 03-26 16:04:45] ax.service.managed_loop: Running optimization trial 8...\n",
      "[INFO 03-26 16:04:46] ax.service.managed_loop: Running optimization trial 9...\n",
      "[INFO 03-26 16:04:46] ax.service.managed_loop: Running optimization trial 10...\n",
      "/Users/xehu/anaconda3/envs/tpm_horserace/lib/python3.11/site-packages/ax/utils/stats/model_fit_stats.py:133: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n",
      "/Users/xehu/anaconda3/envs/tpm_horserace/lib/python3.11/site-packages/ax/utils/stats/model_fit_stats.py:158: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n",
      "[WARNING 03-26 16:04:47] ax.modelbridge.cross_validation: Metric objective was unable to be reliably fit.\n",
      "[WARNING 03-26 16:04:47] ax.service.utils.best_point: Model fit is poor; falling back on raw data for best point.\n",
      "[WARNING 03-26 16:04:47] ax.service.utils.best_point: Model fit is poor and data on objective metric objective is noisy; interpret best points results carefully.\n"
     ]
    }
   ],
   "source": [
    "optimal_params, optimal_q2 = get_optimal_model(X_train, y_train, model_type = \"ElasticNet\", total_trials = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40186159840319036"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.09307539881655244,\n",
       " 'l1_ratio': 0.3760842071755143,\n",
       " 'max_iter': 569,\n",
       " 'selection': 'cyclic'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the evaluation q^2 on the evaluation dataset\n",
    "model_optimal = ElasticNet(alpha = optimal_params[\"alpha\"],\n",
    "                           l1_ratio= optimal_params[\"l1_ratio\"],\n",
    "                           max_iter = optimal_params[\"max_iter\"],\n",
    "                           selection = optimal_params[\"selection\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall Q^2 is now much lower, but we're not seeing much of a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40435696458863113"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_score(model_optimal, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpm_horserace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
